# -*- coding: utf-8 -*-
"""sentiment analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-C1H6alGGbLN1ySccmEIytvFzX3J3DnR
"""

import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense , Embedding ,Flatten,GlobalAveragePooling1D
from tensorflow.keras.preprocessing.text   import Tokenizer
from tensorflow.keras.utils  import pad_sequences

#fix my random init.
seed=0
np.random.seed(seed)
tf.random.set_seed(seed)
random.seed(seed)

#load my data from Hugging face
splits = {'train': 'plain_text/train-00000-of-00001.parquet', 'test': 'plain_text/test-00000-of-00001.parquet', 'unsupervised': 'plain_text/unsupervised-00000-of-00001.parquet'}
df = pd.read_parquet("hf://datasets/stanfordnlp/imdb/" + splits["train"])

#apply my tokenizer to data . and get train data set
max_len=500
num_words=90000
tokenizer = Tokenizer(num_words = num_words,oov_token='<OOV>')
tokenizer.fit_on_texts(df['text'])                                     #this make for each word an ID , will use most num_words only.
sequence= tokenizer.texts_to_sequences(df['text'])                     #this use output from fit_on_texts to give my text into list of IDs [1,2, ....]
x_train = pad_sequences(sequence,maxlen=max_len,padding='post')        #this make all sequences (list) same size ,with fill with zeros .
y_train =np.array(df['label'])

#form module archetecture.
Module=Sequential([
    Embedding(input_dim=num_words, input_length=max_len , output_dim =100),
    #Flatten(),
    GlobalAveragePooling1D(),
    Dense(128,activation='relu'),
    Dense(64,activation='relu'),
    Dense(32,activation='relu'),
    Dense(8,activation='relu'),
    Dense(1,activation='sigmoid')
])
Module.build(input_shape=(None,max_len))

Module.summary()

#make complie setting
Module.compile(
    optimizer='adamax',
    loss='binary_crossentropy',
    metrics=['accuracy']

)
#now fit(train) your module
history=Module.fit(

    x_train,
    y_train,
    epochs=5,
    #batch_size=10000,
    validation_split=.25


)

plt.figure()
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.legend()
plt.title("Accuracy over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.show()

plt.figure()
plt.plot(history.history['loss'], label='Train loss')
plt.plot(history.history['val_loss'], label='Train val_loss')
plt.legend()
plt.title("Loss over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.show()

#Module.save('imdb.keras')
#Module.save('imdb.h5')

#prepare test data
df2=pd.read_parquet("hf://datasets/stanfordnlp/imdb/" + splits["test"])

#apply my tokenizer to test data
max_len=500
num_words=90000
tokenizer2 = Tokenizer(num_words = num_words,oov_token='<OOV>')
tokenizer.fit_on_texts(df2['text'])                                     #this make for each word an ID , will use most num_words only.
sequence2= tokenizer.texts_to_sequences(df2['text'])                     #this use output from fit_on_texts to give my text into list of IDs [1,2, ....]
x_test = pad_sequences(sequence2,maxlen=max_len,padding='post')        #this make all sequences (list) same size ,with fill with zeros .
y_test =np.array(df2['label'])

Module.evaluate(x_test,y_test)

